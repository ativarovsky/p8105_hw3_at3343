---
title: "P8105 Homework 3"
author: "Alice Tivarovsky"
date: "10/6/2019"
output: github_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(viridis)
```

## Problem 1

### Instacart Data

Loading data and generating preview: 
```{r}
library(p8105.datasets)
data("instacart")
view(head(instacart, n = 50))
```


The instacart dataset contains 1,384,617 observations and 15 variables. It catalogues instacart orders from October 2017 in New York City. Key variables include order_id (the order identifier), user_id (the customer identifier), product_id (the product name within each order), and order_dow (the day of the week). The first row, for example, is order_id = 1, meaning it was the first order in October 2017, includes (among 7 other items) the item product_name = Bulgarian Yogurt, which was ordered by customer user_id = 112108 on order_dow = 4 (presumably Wednesday). 

### EDA with Instacart Dataset

Counting # of aisles and aisles most commonly ordered from. 
```{r}
pop_aisle = 
  count(instacart, aisle_id) %>% 
  arrange(desc(n))

pop_aisle
```

There are 134 aisles and the aisle most commonly ordered from is aisle 83 (150609 items). 

Keeping only aisles with n > 10,000, creating a plot that shows the number of items ordered in each aisle. 

```{r}

instacart %>% 
  select(aisle_id, product_name) %>% 
  group_by(aisle_id) %>% 
  summarize(n = n()) %>% 
  filter(n > 10000) %>% 
  ggplot(aes(x = aisle_id, y = n, color = n)) + geom_point() + 
  labs(
    title = "Item Counts by Aisle ID",
    x = "Aisle ID",
    y = "# Items",
    caption = "Instacart Oct 2017 data, limited to aisles with > 10,000 items") +
  scale_x_continuous(
    breaks = c(10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150), 
    labels = c("10", "20", "30", "40", "50", "60", "70", "80", "90", "100", "110", "120", "130", "140", "150"),
    limits = c(0, 150))
  scale_y_continuous(
    breaks = c(10000, 20000, 30000, 40000, 50000, 60000, 70000, 80000, 90000, 100000, 110000, 120000, 130000, 140000, 150000)
  )
 
```

Creating a table showing the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”. 

```{r}
instacart %>% 
  filter(aisle == c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
  select(order_id, aisle, product_name) %>% 
  group_by(aisle, product_name) %>% 
  summarize(n = n()) %>% 
  mutate(product_rank = min_rank(desc(n))) %>% 
  filter(product_rank <= 3) %>% 
  arrange(aisle, product_rank) %>% 
knitr::kable()
```


Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table).

```{r}
instacart %>% 
  filter(product_name == c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
  select(product_name, order_dow, order_hour_of_day) %>% 
  group_by(product_name, order_dow) %>% 
  summarize(mean_hour = mean(order_hour_of_day)) %>% 
  pivot_wider(
    names_from = order_dow,
    values_from = mean_hour
  ) %>% 
  knitr::kable()
```



## Problem 2


Loading BRFSS dataset. 
```{r}
library(p8105.datasets)
data("brfss_smart2010")
view(head(brfss_smart2010, n = 100))
```

The dataset contains 134,203 observations and 23 variables. 

```{r}
brfss_clean = 
  brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  rename(state = locationabbr, county = locationdesc, respondents = sample_size) %>%
  filter(topic == "Overall Health") %>% 
  mutate(response_fac = factor(response, levels = c("Excellent", "Very good", "Good", "Fair", "Poor"))) 

brfss_clean
  
```

Locations by state in 2002: 
```{r}
brfss_clean %>% 
  filter(year == 2002) %>% 
  select(state, county) %>% 
  group_by(state) %>% 
  summarize(n = n_distinct(county)) %>% 
  filter(n >= 7)

```
In 2002, there were 6 states with 7 or more locations represented: CT, FL, MA, NC, NJ, and PA. 

Locations by state in 2010: 
```{r}
brfss_clean %>% 
  filter(year == 2010) %>% 
  select(state, county) %>% 
  group_by(state) %>% 
  summarize(n = n_distinct(county)) %>% 
  filter(n >= 7)
```

In 2010, there are 14 states with 7 or more locations represented: CA, CO, FL, MA, MD, NC, NE, NJ, NY, OH, PA, SC, TX, and WA. 

Construct a dataset that is limited to Excellent responses, and contains, year, state, and a variable that averages the data_value across locations within a state. 

```{r}
brfss_excellent = 
brfss_clean %>% 
  filter(response == "Excellent") %>% 
  select(year, state, county, data_value) %>% 
  drop_na() %>% 
  group_by(year, state) %>% 
  summarize(n = mean(data_value))

brfss_excellent
```
 
Make a “spaghetti” plot of this average value over time within a state (that is, make a plot showing a line for each state across years – the geom_line geometry and group aesthetic will help).

```{r}
brfss_excellent %>% 
  ggplot(aes(x = year, y = n)) + 
  geom_line(aes(group = state, color = state)) +
  viridis::scale_color_viridis(
    name = "State", 
    discrete = TRUE, 
    option = "magma"
  ) + 
  theme_bw()
```

Make a two-panel plot showing, for the years 2006, and 2010, distribution of data_value for responses (“Poor” to “Excellent”) among locations in NY State.

```{r}
brfss_clean %>% 
  filter(year %in% c(2006,2010), state == "NY") %>% 
  select(year, state, county, response, data_value) %>% 
  ggplot(aes(x = response, y = data_value, color = state)) + 
    geom_boxplot() + 
    facet_grid(. ~ year)
  
  
```
 
 
# Problem 3

Lodaing and tidying the accel_data dataset. We see that the original dataset has activity for every minute of the day as its own variable. We want to pivot the data so that minutes are observations and activity is the data. We do this using pivot_longer. We then rename activity to minute and round all the activity values to two digits and convert activity to a numeric vector. Finally, we add a variable to indicate whether the line item corresponds to a weekend observation (day = Saturday or Sunday) or a weekday observation (day = Monday- Friday).

```{r}
accel_data = 
  read.csv("./accel_data.csv") %>% 
  janitor::clean_names() %>% 
  pivot_longer(
    activity_1:activity_1440,
    names_to = "minute",
    names_prefix = "activity_",
    values_to = "activity"
  ) %>% 
  mutate(activity = as.double(activity)) %>% 
  mutate(activity = round(activity, digits = 2)) %>% 
  mutate(minute = as.numeric(minute)) %>% 
  mutate(wkday_wkend = if_else(day %in% c("Saturday", "Sunday"), "weekend", "weekday"))

accel_data
# change activity.1 to minute_1
# round to 2 decimals
# add variable to indicate weekend/weekday

```

5 weeks, day_id runs 1 - 35

 

