---
title: "P8105 Homework 3"
author: "Alice Tivarovsky"
date: "10/6/2019"
output: github_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(viridis)
```

## Problem 1

### Instacart Data

Loading data and generating preview: 
```{r dataframe_1}
library(p8105.datasets)
data("instacart")
view(head(instacart, n = 50))

instacart
```


The instacart dataset contains 1,384,617 observations and 15 variables. It catalogues instacart orders from October 2017 in New York City. Key variables include order_id (the order identifier), user_id (the customer identifier), product_id (the product name within each order), and order_dow (the day of the week). The first row, for example, is order_id = 1, meaning it was the first order in October 2017, includes (among 7 other items) the item product_name = Bulgarian Yogurt, which was ordered by customer user_id = 112108 on order_dow = 4 (presumably Wednesday). 

### EDA with Instacart Dataset

Counting # of aisles and aisles most commonly ordered from:
```{r dataframe_2}
pop_aisle = 
  count(instacart, aisle_id) %>% 
  arrange(desc(n))

pop_aisle
```

There are 134 aisles and the aisle most commonly ordered from is aisle 83 (150609 items). 

Keeping only aisles with n > 10,000, creating a plot that shows the number of items ordered in each aisle:

```{r dataframe_3}

instacart %>% 
  select(aisle_id, product_name) %>% 
  group_by(aisle_id) %>% 
  summarize(n = n()) %>% 
  filter(n > 10000) %>% 
  ggplot(aes(x = aisle_id, y = n, color = n)) + geom_point() + 
  labs(
    title = "Item Counts by Aisle ID",
    x = "Aisle ID",
    y = "# Items",
    caption = "Instacart Oct 2017 data, limited to aisles with > 10,000 items") +
  scale_x_continuous(
    breaks = c(10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150), 
    labels = c("10", "20", "30", "40", "50", "60", "70", "80", "90", "100", "110", "120", "130", "140", "150"),
    limits = c(0, 150))
  scale_y_continuous(
    breaks = c(10000, 20000, 30000, 40000, 50000, 60000, 70000, 80000, 90000, 100000, 110000, 120000, 130000, 140000, 150000)
  )
 
```


Creating a table displaying the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”:

```{r dataframe_4}
instacart %>% 
  filter(aisle == c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
  select(order_id, aisle, product_name) %>% 
  group_by(aisle, product_name) %>% 
  summarize(n = n()) %>% 
  mutate(product_rank = min_rank(desc(n))) %>% 
  filter(product_rank <= 3) %>% 
  arrange(aisle, product_rank) %>% 
knitr::kable()
```


Creating a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week: 

```{r dataframe_5}
instacart %>% 
  filter(product_name == c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
  select(product_name, order_dow, order_hour_of_day) %>% 
  group_by(product_name, order_dow) %>% 
  summarize(mean_hour = mean(order_hour_of_day)) %>% 
  pivot_wider(
    names_from = order_dow,
    values_from = mean_hour
  ) %>% 
  knitr::kable()
```



## Problem 2


Loading BRFSS dataset:

```{r dataframe_6}
library(p8105.datasets)
data("brfss_smart2010")
view(head(brfss_smart2010, n = 100))

brfss_smart2010
```

The dataset contains 134,203 observations and 23 variables. 

Cleaning data, focusing on "Overall Health", taking responses from "Excellent" to "Poor", and organizing responses as factors ranging from "Excellent" to "Poor": 

```{r dataframe_7}
brfss_clean = 
  brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  rename(state = locationabbr, county = locationdesc, respondents = sample_size) %>%
  filter(topic == "Overall Health") %>% 
  mutate(response_fac = factor(response, levels = c("Excellent", "Very good", "Good", "Fair", "Poor"))) 

brfss_clean
```

Locations by state in 2002: 
```{r dataframe_8}
brfss_clean %>% 
  filter(year == 2002) %>% 
  select(state, county) %>% 
  group_by(state) %>% 
  summarize(n = n_distinct(county)) %>% 
  filter(n >= 7)
```
In 2002, there were 6 states with 7 or more locations represented: CT, FL, MA, NC, NJ, and PA. 

Locations by state in 2010: 
```{r dataframe_9}
brfss_clean %>% 
  filter(year == 2010) %>% 
  select(state, county) %>% 
  group_by(state) %>% 
  summarize(n = n_distinct(county)) %>% 
  filter(n >= 7)
```

In 2010, there are 14 states with 7 or more locations represented: CA, CO, FL, MA, MD, NC, NE, NJ, NY, OH, PA, SC, TX, and WA. 

Constructing a dataset limited to Excellent responses, and containing year, state, and a variable that averages the data_value across locations within a state:

```{r dataframe_10}
brfss_excellent = 
brfss_clean %>% 
  filter(response == "Excellent") %>% 
  select(year, state, county, data_value) %>% 
  drop_na() %>% 
  group_by(year, state) %>% 
  summarize(n = mean(data_value))

brfss_excellent
```
 
Creating spaghetti plot of  average value over time within a state:

```{r dataframe_11}
brfss_excellent %>% 
  ggplot(aes(x = year, y = n)) + 
  geom_line(aes(group = state, color = state)) +
  viridis::scale_color_viridis(
    name = "State", 
    discrete = TRUE, 
    option = "magma"
  ) + 
   labs(
    title = "Excellent Responses in General Health",
    x = "Year",
    y = "Proportion Excellent", 
    caption = "Proportion Excellent indicates the proportion of those who responded with excellent to self-rated general health"
   ) +
  theme_bw()
```


Creating a two-panel plot showing, for the years 2006, and 2010, distribution of data_value for responses (“Poor” to “Excellent”) among locations in NY State:

```{r dataframe_12}
brfss_clean %>% 
  filter(year %in% c(2006,2010), state == "NY") %>% 
  select(year, state, county, response, data_value) %>% 
  ggplot(aes(x = response, y = data_value, color = state)) + 
    geom_boxplot() + 
    facet_grid(. ~ year) +  
  labs(
    title = "Distribution of Self-Rated Health in NY",
    x = "Response",
    y = "Proportion"
  ) + 
  theme_bw()
  
```
 
 
## Problem 3

Lodaing and tidying the accel_data dataset. We see that the original dataset has activity for every minute of the day as its own variable. We want to pivot the data so that minutes are observations (rows) and activity is the data (values). We do this using pivot_longer. We then rename activity to minute and round all the activity values to two digits and convert activity to a numeric vector. Finally, we add a variable to indicate whether the line item corresponds to a weekend observation (day = Saturday or Sunday) or a weekday observation (day = Monday - Friday).

```{r dataframe_13}
accel_data = 
  read.csv("./accel_data.csv") %>% 
  janitor::clean_names() %>% 
  pivot_longer(
    activity_1:activity_1440,
    names_to = "minute",
    names_prefix = "activity_",
    values_to = "activity"
  ) %>% 
  mutate(activity = as.double(activity)) %>% 
  mutate(activity = round(activity, digits = 2)) %>% 
  mutate(minute = as.numeric(minute)) %>% 
  mutate(wkday_wkend = if_else(day %in% c("Saturday", "Sunday"), "weekend", "weekday"))

accel_data
```


Now we create a total activity variable for each of the 35 days represented, and create a table of the resulting dataset. 

```{r dataframe_14}
accel_data %>% 
  group_by(day_id, day) %>% 
  summarize(n = sum(activity)) %>% 
  knitr::kable()

```

It is difficult to detect any trend just looking at the table. We see that in general, though, Saturdays appear to be the least active day. 

We next plot activity as a function of the minute of each hour of the day. We aggregate over hour and sum over activity, segregating the data by day of week.  

```{r dataframe_15}
  accel_data %>% 
  mutate(hour = minute %/% 60) %>% 
  group_by(day, hour) %>% 
  summarize(n = sum(activity)) %>% 
  ggplot(aes(x = hour, y = n)) +
    geom_line(aes(color = day)) +
    labs(
    x = "Hour", 
    y = "Activity",
    title = "Activity by Hour") +
    scale_y_continuous(
    breaks = c(50000, 100000, 150000, 200000, 250000, 300000)
  )
  
```

 
Based on this graph, we see that in general for this individual, the highest activity points are between hour 17 and 21 (i.e. between 5pm and 9pm). We see that Sunday deviates from this pattern, activity peaking around 11am and declining throughout the day. We see that the lowest activity points are between hour 0 and hour 5, and that activity initiates around 6am, with the exception of Wednesday and Thursday, when it initiates before 5am. 